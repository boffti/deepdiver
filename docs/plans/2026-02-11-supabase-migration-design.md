# Supabase Migration Design

**Date:** 2026-02-11
**Status:** Approved
**Goal:** Migrate DeepDiver from Google Sheets + Local JSON to full Supabase cloud architecture

---

## Executive Summary

This design eliminates the Google Sheets dependency and local JSON file storage, migrating the entire CANSLIM Scanner system to Supabase. Wilson (the AI agent) will autonomously generate market scans using external market data APIs, and all application state will live in Supabase PostgreSQL with real-time WebSocket updates to the dashboard.

**Key Outcomes:**
- âœ… No external dependencies (gog CLI removed)
- âœ… Stateless Raspberry Pi deployment
- âœ… Full cloud persistence with remote monitoring
- âœ… Real-time dashboard updates via WebSockets
- âœ… Autonomous scan generation by Wilson
- âœ… Docker support for local testing

---

## Architecture Decisions

### 1. Complete Google Sheets Replacement
**Decision:** Eliminate Google Sheets entirely as a data source.

**Rationale:**
- Aligns with "Cloud State" philosophy from project_vision.md
- Removes dependency on gog CLI and service account setup
- Enables true autonomous operation on Raspberry Pi
- Simplifies deployment and reduces failure points

**Alternative Considered:** Hybrid approach (read from Sheets, cache to Supabase) - rejected as it maintains external dependency and complexity.

### 2. Wilson Generates Scans Autonomously
**Decision:** Wilson agent uses market data API tools to generate CANSLIM scans.

**Rationale:**
- True autonomous trading system (no human input required)
- Scheduled scans run 24/7 on the Pi
- Aligns with "autonomous trading swarm" vision
- Agent has full context of positions, watchlist, and market data

**Market Data API Options:**
- polygon.io (recommended, free tier: 5 calls/min)
- finnhub.io (free tier available)
- Alpha Vantage (fallback option)

### 3. Migrate All Data to Supabase
**Decision:** All local JSON files (settings, alerts, earnings, positions, calls, routines, history) move to Supabase tables.

**Rationale:**
- Stateless Pi - can be destroyed and rebuilt instantly
- Remote monitoring via Supabase dashboard
- Real-time collaboration potential (multiple devices)
- Automatic backups and point-in-time recovery
- Query historical data with SQL

**Trade-off:** Increased database schema complexity, but gains outweigh costs for a cloud-first architecture.

### 4. Start Fresh (No Migration Script)
**Decision:** Deploy with empty Supabase tables, no data migration from existing JSON files.

**Rationale:**
- Simpler deployment path
- Clean slate for production system
- Existing JSON data likely minimal or test data
- Reduces migration risk and complexity

### 5. Real-time Dashboard Updates
**Decision:** Use Supabase Realtime WebSocket subscriptions for live UI updates.

**Rationale:**
- Essential for monitoring autonomous trading bot
- See Wilson's decisions in real-time
- Instant alert notifications
- No polling overhead or stale cache issues
- Better UX for 24/7 system monitoring

**Trade-off:** Adds frontend WebSocket code complexity, but critical for live trading visibility.

### 6. Docker for Local Testing
**Decision:** Provide simple Dockerfile + docker-compose.yml for portable testing.

**Rationale:**
- Test full stack without Raspberry Pi access
- Consistent environment between dev and production
- Easy onboarding for future developers
- Portable across machines

**Implementation:** Single-stage build (simple), volume mounts for hot reload during development.

---

## Database Schema

### Core Tables

#### 1. scans
Stores CANSLIM scan metadata (replaces Google Sheets rows 0-2).

```sql
CREATE TABLE scans (
  id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  created_at TIMESTAMPTZ DEFAULT NOW() NOT NULL,
  scan_time TIMESTAMPTZ NOT NULL,
  market_regime TEXT NOT NULL,  -- Confirmed, Rally Attempt, Under Pressure, Correction
  dist_days TEXT,
  buy_ok TEXT,
  account_balance NUMERIC,
  risk_per_trade NUMERIC,
  actionable_count INT,
  metadata JSONB  -- Flexible for additional scan-level data
);
```

#### 2. scan_stocks
Stores individual stock candidates from each scan (replaces Google Sheets rows 5+).

```sql
CREATE TABLE scan_stocks (
  id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  scan_id BIGINT REFERENCES scans(id) ON DELETE CASCADE,
  ticker TEXT NOT NULL,
  pivot NUMERIC,
  stop NUMERIC,
  rs_rating NUMERIC,
  comp_rating NUMERIC,
  eps_rating NUMERIC,
  setup_type TEXT,
  notes TEXT,
  metadata JSONB,  -- Flexible for any extra columns
  created_at TIMESTAMPTZ DEFAULT NOW()
);
CREATE INDEX idx_scan_stocks_scan_id ON scan_stocks(scan_id);
CREATE INDEX idx_scan_stocks_ticker ON scan_stocks(ticker);
```

**Design Note:** Normalized two-table design allows querying historical scans efficiently and maintains referential integrity with CASCADE delete.

#### 3. settings
Key-value store for application configuration (replaces settings.json).

```sql
CREATE TABLE settings (
  key TEXT PRIMARY KEY,
  value JSONB NOT NULL,
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

INSERT INTO settings (key, value) VALUES
  ('account_equity', '100000'),
  ('risk_pct', '0.01'),
  ('max_positions', '6');
```

**Design Note:** JSONB allows flexible value types (numbers, strings, objects) without schema changes.

#### 4. alerts
Price alerts with trigger tracking (replaces alerts.json).

```sql
CREATE TABLE alerts (
  id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  ticker TEXT NOT NULL,
  condition TEXT NOT NULL,  -- 'above' or 'below'
  price NUMERIC NOT NULL,
  triggered BOOLEAN DEFAULT FALSE,
  created_at TIMESTAMPTZ DEFAULT NOW()
);
```

#### 5. earnings
Earnings calendar by ticker (replaces earnings.json).

```sql
CREATE TABLE earnings (
  ticker TEXT PRIMARY KEY,
  earnings_date DATE NOT NULL,
  updated_at TIMESTAMPTZ DEFAULT NOW()
);
```

#### 6. positions
Stock positions tracking (replaces positions.json).

```sql
CREATE TABLE positions (
  id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  ticker TEXT NOT NULL,
  account TEXT DEFAULT 'default',
  trade_type TEXT DEFAULT 'long',
  entry_date DATE NOT NULL,
  entry_price NUMERIC NOT NULL,
  shares INT NOT NULL,
  cost_basis NUMERIC NOT NULL,
  stop_price NUMERIC,
  target_price NUMERIC,
  setup_type TEXT,
  status TEXT DEFAULT 'open',
  close_date DATE,
  close_price NUMERIC,
  pnl NUMERIC,
  notes TEXT,
  created_at TIMESTAMPTZ DEFAULT NOW()
);
CREATE INDEX idx_positions_status ON positions(status);
```

#### 7. covered_calls
Covered call trades (replaces covered_calls.json).

```sql
CREATE TABLE covered_calls (
  id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  ticker TEXT NOT NULL,
  sell_date DATE NOT NULL,
  expiry DATE NOT NULL,
  strike NUMERIC NOT NULL,
  contracts INT NOT NULL,
  premium_per_contract NUMERIC NOT NULL,
  premium_total NUMERIC NOT NULL,
  delta NUMERIC,
  stock_price_at_sell NUMERIC,
  status TEXT DEFAULT 'open',
  close_date DATE,
  close_price NUMERIC,
  pnl NUMERIC,
  notes TEXT,
  created_at TIMESTAMPTZ DEFAULT NOW()
);
```

#### 8. routines
Daily trading routines (replaces routines/*.json files).

```sql
CREATE TABLE routines (
  id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  date DATE NOT NULL,
  routine_type TEXT NOT NULL,  -- 'premarket' or 'postclose'
  data JSONB NOT NULL,  -- Flexible routine fields
  updated_at TIMESTAMPTZ DEFAULT NOW(),
  UNIQUE(date, routine_type)
);
```

**Design Note:** UNIQUE constraint prevents duplicate routines for same date/type. JSONB allows flexible form fields without schema changes.

#### 9. journal
Agent logs and memory (already exists, from project_vision.md).

```sql
CREATE TABLE journal (
  id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  created_at TIMESTAMPTZ DEFAULT NOW() NOT NULL,
  agent_name TEXT NOT NULL,     -- e.g. "Wilson", "Scanner"
  category TEXT NOT NULL,       -- e.g. "Trade", "Error", "Thinking", "Signal"
  content TEXT NOT NULL,        -- The actual message or JSON payload
  meta JSONB                    -- Optional structured data (ticker, price, etc)
);
```

#### 10. watchlist
Stocks being monitored (from project_vision.md).

```sql
CREATE TABLE watchlist (
  ticker TEXT PRIMARY KEY,
  status TEXT NOT NULL,         -- "Watching", "Long", "Short"
  sentiment_score NUMERIC,      -- 0 to 100
  last_updated TIMESTAMPTZ DEFAULT NOW()
);
```

#### 11. bot_config
Remote control switches (from project_vision.md).

```sql
CREATE TABLE bot_config (
  key TEXT PRIMARY KEY,
  value TEXT,
  description TEXT
);

INSERT INTO bot_config (key, value, description)
VALUES ('trading_enabled', 'true', 'Master switch for trade execution');
```

---

## Wilson Agent Tools

Wilson needs new tools to replace Google Sheets data fetching and enable autonomous scanning.

### New Tools (app/agents/tools.py)

#### 1. fetch_market_data
```python
@tool
def fetch_market_data(tickers: list[str]) -> str:
    """Fetches real-time price, volume, RS rating data from market API.

    Args:
        tickers: List of stock symbols to analyze

    Returns:
        JSON string with price data, technical indicators, RS rankings

    Implementation:
        - Uses polygon.io API (or finnhub.io)
        - Calculates RS rating (relative strength vs market)
        - Returns formatted data for CANSLIM analysis
    """
```

#### 2. write_scan_results
```python
@tool
def write_scan_results(
    market_regime: str,
    stocks: list[dict],
    metadata: dict
) -> str:
    """Writes CANSLIM scan results to Supabase.

    Args:
        market_regime: Current market condition
        stocks: List of stock candidates with CANSLIM metrics
        metadata: Scan-level data (dist_days, buy_ok, actionable_count)

    Returns:
        Confirmation with scan_id

    Implementation:
        - Creates scan record in `scans` table
        - Bulk inserts stocks into `scan_stocks` table
        - Returns scan_id for reference
    """
```

#### 3. get_current_positions
```python
@tool
def get_current_positions() -> str:
    """Retrieves all open positions from Supabase.

    Returns:
        Formatted list of open positions for decision-making
    """
```

#### 4. get_watchlist
```python
@tool
def get_watchlist() -> str:
    """Gets stocks currently being monitored.

    Returns:
        Tickers with sentiment scores and status
    """
```

#### 5. update_position
```python
@tool
def update_position(position_id: int, updates: dict) -> str:
    """Updates position (change stop, close, etc.).

    Args:
        position_id: Position ID to update
        updates: Fields to update (stop_price, status, close_price, etc.)

    Returns:
        Confirmation message

    Implementation:
        - Writes to `positions` table
        - Calculates PnL if closing position
    """
```

#### 6. check_alerts
```python
@tool
def check_alerts() -> str:
    """Checks if any price alerts have triggered.

    Returns:
        List of triggered alerts

    Implementation:
        - Reads alerts from Supabase
        - Fetches current prices via market data API
        - Updates triggered status if condition met
    """
```

#### 7. add_to_watchlist
```python
@tool
def add_to_watchlist(ticker: str, status: str, score: float) -> str:
    """Adds or updates a stock in the watchlist.

    Args:
        ticker: Stock symbol
        status: 'Watching', 'Long', or 'Short'
        score: Sentiment score 0-100

    Returns:
        Confirmation message
    """
```

### Updated Scheduled Tasks (app/tasks.py)

```python
@scheduler.task('cron', id='morning_scan', day_of_week='mon-fri', hour=8, minute=30)
def task_morning_scan():
    """Wilson performs autonomous CANSLIM scan at 8:30 AM ET."""
    prompt = """It's 8:30 AM ET. Perform your morning CANSLIM scan:
    1. Use fetch_market_data() to get top RS stocks from your screener universe
    2. Analyze each stock against CANSLIM criteria:
       - C: Current quarterly earnings (25%+ growth)
       - A: Annual earnings (25%+ growth over 3 years)
       - N: New highs, products, management
       - S: Supply & demand (volume, institutional buying)
       - L: Leader or laggard (RS rating > 80)
       - I: Institutional sponsorship
       - M: Market direction (confirm market regime)
    3. Write results using write_scan_results()
    4. Check alerts with check_alerts()
    5. Review positions with get_current_positions()
    6. Log your analysis and recommendations to journal
    """

    try:
        response = wilson.run(prompt)
        print(f"âœ“ Morning scan completed: {response.text}")
    except Exception as e:
        print(f"âœ— Morning scan failed: {e}")
        # Log error to journal
        log_journal("System", "Error", f"Morning scan failed: {e}")
```

---

## API Layer Refactoring

### app/dashboard/utils.py - Complete Rewrite

**Remove:**
- `fetch_sheet_data()` - Google Sheets integration
- `parse_sheet_data()` - Sheet parsing logic
- `get_cached_data()` - Cache management
- `load_json_file()` - JSON file reading
- `save_json_file()` - JSON file writing with locking
- Global `cache` dict
- All file path constants (ALERTS_FILE, SETTINGS_FILE, etc.)

**Add:**

```python
from app.extensions import supabase

def get_latest_scan():
    """Get most recent CANSLIM scan with stocks."""
    result = supabase.table('scans') \
        .select('*, scan_stocks(*)') \
        .order('created_at', desc=True) \
        .limit(1) \
        .execute()
    return result.data[0] if result.data else None

def get_scan_by_id(scan_id):
    """Get specific scan with stocks."""
    result = supabase.table('scans') \
        .select('*, scan_stocks(*)') \
        .eq('id', scan_id) \
        .single() \
        .execute()
    return result.data

def get_all_scans(limit=50):
    """Get historical scans (replaces history/*.json files)."""
    result = supabase.table('scans') \
        .select('id, created_at, scan_time, market_regime, actionable_count') \
        .order('created_at', desc=True) \
        .limit(limit) \
        .execute()
    return result.data

def get_all_positions(status='open'):
    """Get positions filtered by status."""
    query = supabase.table('positions').select('*')
    if status:
        query = query.eq('status', status)
    return query.order('entry_date', desc=True).execute().data

def get_settings():
    """Get all settings as dict."""
    result = supabase.table('settings').select('*').execute()
    return {row['key']: row['value'] for row in result.data}

def update_setting(key, value):
    """Update a single setting."""
    supabase.table('settings') \
        .upsert({'key': key, 'value': value, 'updated_at': 'now()'}) \
        .execute()

def get_all_alerts():
    """Get all alerts."""
    return supabase.table('alerts') \
        .select('*') \
        .order('created_at', desc=True) \
        .execute().data

def add_alert(ticker, condition, price):
    """Add new alert."""
    result = supabase.table('alerts').insert({
        'ticker': ticker.upper(),
        'condition': condition,
        'price': float(price),
        'triggered': False
    }).execute()
    return result.data[0]

def delete_alert(alert_id):
    """Delete alert by ID."""
    supabase.table('alerts').delete().eq('id', alert_id).execute()

# Similar helpers for earnings, positions, calls, routines...
```

**Key Changes:**
- All functions query Supabase directly (no file I/O)
- No caching layer (Supabase queries are fast ~50ms)
- Simpler error handling (database errors propagate)
- ~60% less code (removed file locking, cache logic, parsing)

### app/dashboard/routes.py - Route Updates

#### /api/data - Main scan data endpoint
```python
@bp.route('/api/data')
def api_data():
    """Get latest scan data (no caching)."""
    try:
        scan = get_latest_scan()
        if not scan:
            return jsonify({'error': 'No scans found'}), 404

        # Calculate position sizing (same logic as before)
        settings = get_settings()
        account_equity = float(settings.get('account_equity', 100000))
        risk_pct = float(settings.get('risk_pct', 0.01))
        risk_per_trade = account_equity * risk_pct

        for stock in scan['scan_stocks']:
            pivot = float(stock.get('pivot', 0))
            stop = float(stock.get('stop', 0))
            if pivot > stop > 0:
                risk_per_share = pivot - stop
                shares = int(risk_per_trade / risk_per_share)
                stock['shares'] = shares
                stock['cost'] = shares * pivot

        return jsonify(scan)
    except Exception as e:
        return jsonify({'error': str(e)}), 500
```

#### /api/history - Historical scans
```python
@bp.route('/api/history')
def get_history():
    """List all historical scans."""
    try:
        scans = get_all_scans(limit=100)
        return jsonify(scans)
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@bp.route('/api/history/<int:scan_id>')
def get_historical_scan(scan_id):
    """Get specific historical scan."""
    try:
        scan = get_scan_by_id(scan_id)
        return jsonify(scan)
    except Exception as e:
        return jsonify({'error': str(e)}), 500
```

#### /api/settings - Settings management
```python
@bp.route('/api/settings', methods=['GET'])
def get_settings_api():
    """Get scanner settings from Supabase."""
    return jsonify(get_settings())

@bp.route('/api/settings', methods=['POST'])
def update_settings_api():
    """Update settings in Supabase."""
    data = request.json
    for key in ['account_equity', 'risk_pct', 'max_positions']:
        if key in data:
            update_setting(key, data[key])
    return jsonify(get_settings())
```

#### /api/alerts - Alert management
```python
@bp.route('/api/alerts', methods=['GET'])
def get_alerts():
    """Get all alerts."""
    return jsonify(get_all_alerts())

@bp.route('/api/alerts', methods=['POST'])
def add_alert_api():
    """Add new alert."""
    try:
        data = request.json
        # Validation (same as before)
        ticker = data['ticker'].strip().upper()
        condition = data['condition']
        price = float(data['price'])

        alert = add_alert(ticker, condition, price)
        return jsonify(alert), 201
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@bp.route('/api/alerts/<int:alert_id>', methods=['DELETE'])
def delete_alert_api(alert_id):
    """Delete alert."""
    try:
        delete_alert(alert_id)
        return jsonify({'ok': True})
    except Exception as e:
        return jsonify({'error': str(e)}), 500
```

**Similar updates for:** `/api/positions`, `/api/calls`, `/api/earnings`, `/api/routines`

**Key Changes:**
- No `/api/refresh` needed (no cache to clear)
- Direct Supabase queries in routes
- Error handling simplified
- All JSON file operations removed

---

## Real-time Frontend

### Supabase Client Setup (app/templates/index.html)

Add Supabase JS SDK and initialize real-time subscriptions:

```html
<!-- In <head> -->
<script src="https://cdn.jsdelivr.net/npm/@supabase/supabase-js@2"></script>

<script>
  // Initialize Supabase client
  const supabase = window.supabase.createClient(
    '{{ config.SUPABASE_URL }}',
    '{{ config.SUPABASE_ANON_KEY }}'
  );

  // Real-time subscription setup
  function setupRealtimeSubscriptions() {

    // 1. Listen for new scans
    supabase
      .channel('scans-channel')
      .on('postgres_changes',
          { event: 'INSERT', schema: 'public', table: 'scans' },
          (payload) => {
            console.log('ðŸ” New scan detected:', payload.new);
            showNotification('New CANSLIM scan available');
            refreshScanData();
          })
      .subscribe();

    // 2. Listen for position changes
    supabase
      .channel('positions-channel')
      .on('postgres_changes',
          { event: '*', schema: 'public', table: 'positions' },
          (payload) => {
            console.log('ðŸ“Š Position updated:', payload);
            refreshPositionsPanel();
          })
      .subscribe();

    // 3. Listen for triggered alerts
    supabase
      .channel('alerts-channel')
      .on('postgres_changes',
          { event: 'UPDATE', schema: 'public', table: 'alerts' },
          (payload) => {
            if (payload.new.triggered && !payload.old.triggered) {
              showNotification(`ðŸš¨ Alert: ${payload.new.ticker} ${payload.new.condition} $${payload.new.price}`);
              playAlertSound();
            }
          })
      .subscribe();

    // 4. Listen for Wilson's journal entries
    supabase
      .channel('journal-channel')
      .on('postgres_changes',
          { event: 'INSERT', schema: 'public', table: 'journal' },
          (payload) => {
            if (payload.new.category === 'Trade') {
              appendToActivityFeed(payload.new);
            }
          })
      .subscribe();
  }

  // Helper: Refresh scan data
  async function refreshScanData() {
    const response = await fetch('/api/data');
    const data = await response.json();
    updateScanTable(data.scan_stocks);
    updateMarketRegime(data.market_regime);
  }

  // Helper: Show notification
  function showNotification(message) {
    // Browser notification
    if ('Notification' in window && Notification.permission === 'granted') {
      new Notification('DeepDiver', { body: message });
    }

    // In-app toast
    const toast = document.createElement('div');
    toast.className = 'toast-notification';
    toast.textContent = message;
    document.body.appendChild(toast);
    setTimeout(() => toast.remove(), 5000);
  }

  // Helper: Play alert sound
  function playAlertSound() {
    const audio = new Audio('/static/alert.mp3');
    audio.play().catch(e => console.log('Audio play failed:', e));
  }

  // Initialize on page load
  document.addEventListener('DOMContentLoaded', () => {
    setupRealtimeSubscriptions();
    refreshScanData();

    // Request notification permission
    if ('Notification' in window && Notification.permission === 'default') {
      Notification.requestPermission();
    }
  });

  // Connection status monitoring
  supabase
    .channel('system')
    .on('system', { event: 'connected' }, () => {
      console.log('âœ“ Real-time connected');
      document.getElementById('status-indicator').className = 'online';
    })
    .on('system', { event: 'disconnected' }, () => {
      console.warn('âœ— Real-time disconnected');
      document.getElementById('status-indicator').className = 'offline';
    })
    .subscribe();
</script>
```

### UI Updates

Add status indicator to show WebSocket connection:

```html
<div id="status-indicator" class="offline" title="Real-time connection status"></div>

<style>
  #status-indicator {
    position: fixed;
    top: 10px;
    right: 10px;
    width: 12px;
    height: 12px;
    border-radius: 50%;
    z-index: 1000;
  }
  #status-indicator.online {
    background: #00ff00;
    box-shadow: 0 0 10px #00ff00;
  }
  #status-indicator.offline {
    background: #ff0000;
    box-shadow: 0 0 10px #ff0000;
  }

  .toast-notification {
    position: fixed;
    bottom: 20px;
    right: 20px;
    background: #333;
    color: white;
    padding: 15px 20px;
    border-radius: 8px;
    box-shadow: 0 4px 12px rgba(0,0,0,0.3);
    z-index: 1001;
    animation: slideIn 0.3s ease;
  }

  @keyframes slideIn {
    from { transform: translateX(400px); opacity: 0; }
    to { transform: translateX(0); opacity: 1; }
  }
</style>
```

**Features:**
- Auto-update on new scans (no refresh button needed)
- Browser notifications for alerts and new scans
- Live activity feed for Wilson's trading decisions
- Connection status indicator (green = connected, red = offline)
- Audio alerts for triggered price alerts

---

## Docker Configuration

### Dockerfile

```dockerfile
FROM python:3.11-slim

# Set working directory
WORKDIR /app

# Install uv
RUN pip install uv

# Copy dependency files
COPY pyproject.toml uv.lock ./

# Install dependencies
RUN uv sync --frozen

# Copy application code
COPY . .

# Expose port
EXPOSE 8080

# Run the application
CMD ["uv", "run", "run.py"]
```

### docker-compose.yml

```yaml
version: '3.8'

services:
  deepdiver:
    build: .
    container_name: deepdiver
    ports:
      - "8080:8080"
    env_file:
      - .env
    environment:
      - FLASK_ENV=development
      - PORT=8080
    volumes:
      # Mount code for hot reload during development
      - ./app:/app/app
      - ./run.py:/app/run.py
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
```

### .dockerignore

```
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
.venv/
app/data/
*.json
.git/
.gitignore
.vscode/
.idea/
*.swp
.env.local
.env.production
*.md
docs/
*.log
```

### Usage

```bash
# Build and start
docker-compose up -d

# View logs
docker-compose logs -f deepdiver

# Test Wilson scan
docker-compose exec deepdiver uv run python -c "
from app.agents.wilson import wilson
wilson.run('Test scan: AAPL, MSFT, GOOGL')
"

# Check scheduled jobs
docker-compose exec deepdiver uv run python -c "
from app.extensions import scheduler
print(scheduler.get_jobs())
"

# Stop
docker-compose down
```

---

## Deployment Process

### Phase 1: Supabase Setup

1. **Create Supabase project** at supabase.com
2. **Run SQL schema** from Section 1 in Supabase SQL Editor
3. **Enable Realtime** for tables:
   - Database â†’ Replication â†’ Enable for: scans, scan_stocks, positions, alerts, journal
4. **Get credentials:**
   - Settings â†’ API â†’ Project URL
   - Settings â†’ API â†’ anon public key (for frontend)
   - Settings â†’ API â†’ service_role key (for backend)
5. **Configure RLS (optional for MVP):**
   - Disable RLS for development
   - Production: Add policies to restrict access

### Phase 2: Code Changes

1. **Update .env:**
```bash
# Remove (no longer needed)
# GOOGLE_SHEET_ID=...
# GOG_ACCOUNT=...
# SHEET_RANGE=...

# Add
SUPABASE_URL=https://xxxxx.supabase.co
SUPABASE_KEY=eyJhbGc...  # service_role key
SUPABASE_ANON_KEY=eyJhbGc...  # anon public key
POLYGON_API_KEY=your_key_here  # or FINNHUB_API_KEY
```

2. **Update dependencies:**
```bash
uv add polygon-api-client  # or finnhub-python
uv sync
```

3. **Code file changes:**
- `app/dashboard/utils.py`: Complete rewrite (remove JSON/Sheets, add Supabase)
- `app/dashboard/routes.py`: Update all routes to use Supabase helpers
- `app/agents/tools.py`: Add 7 new tools (fetch_market_data, write_scan_results, etc.)
- `app/tasks.py`: Update task_morning_briefing prompt
- `app/templates/index.html`: Add Supabase JS client and subscriptions
- `app/extensions.py`: Expose SUPABASE_ANON_KEY to templates

4. **Delete obsolete code:**
```bash
rm -rf app/data/  # No more local JSON
```

### Phase 3: Local Testing with Docker

```bash
# Build
docker-compose build

# Start
docker-compose up -d

# Test health
curl http://localhost:8080/api/health

# Test Wilson scan (manual trigger)
docker-compose exec deepdiver uv run python -c "
from app.agents.wilson import wilson
response = wilson.run('Perform test CANSLIM scan on AAPL, NVDA, TSLA')
print(response.text)
"

# Check Supabase dashboard
# - Verify new scan in 'scans' table
# - Verify stocks in 'scan_stocks' table
# - Verify Wilson logs in 'journal' table

# Test real-time updates
# - Open http://localhost:8080 in browser
# - Manually insert a scan in Supabase SQL editor
# - Dashboard should auto-update (check console for "New scan detected")

# Test API endpoints
curl http://localhost:8080/api/data
curl http://localhost:8080/api/settings
curl -X POST http://localhost:8080/api/alerts \
  -H "Content-Type: application/json" \
  -d '{"ticker":"AAPL","condition":"above","price":150}'
```

### Phase 4: Raspberry Pi Deployment

```bash
# SSH to Pi
ssh pi@raspberrypi.local

# Pull updated code
cd ~/deepdiver
git pull

# Update .env with production Supabase credentials
nano .env

# Sync dependencies
uv sync

# Restart service
sudo systemctl restart deepdiver
sudo systemctl status deepdiver

# Monitor logs
journalctl -u deepdiver -f

# Verify scheduled tasks
# Check Supabase journal table at 8:30 AM ET for morning scan logs
```

---

## Environment Variables

### Before Migration
```bash
GOOGLE_SHEET_ID=abc123
GOG_ACCOUNT=service-account@project.iam.gserviceaccount.com
SHEET_RANGE='Main'!A1:W50
CACHE_DURATION=300
OPENROUTER_API_KEY=sk-or-...
SUPABASE_URL=https://xxx.supabase.co
SUPABASE_KEY=eyJ...
PORT=8080
FLASK_ENV=development
```

### After Migration
```bash
# Removed: GOOGLE_SHEET_ID, GOG_ACCOUNT, SHEET_RANGE, CACHE_DURATION

# Added:
SUPABASE_URL=https://xxx.supabase.co
SUPABASE_KEY=eyJ...  # service_role key (backend)
SUPABASE_ANON_KEY=eyJ...  # anon key (frontend)
POLYGON_API_KEY=your_key  # OR FINNHUB_API_KEY

# Unchanged:
OPENROUTER_API_KEY=sk-or-...
PORT=8080
FLASK_ENV=development
```

---

## Testing Checklist

### Supabase Connection
- [ ] Health endpoint returns Supabase connected
- [ ] Can query settings table
- [ ] Can insert test alert
- [ ] Can read journal entries

### Wilson Agent
- [ ] Manual scan trigger works
- [ ] Scan results written to scans table
- [ ] Stocks written to scan_stocks table
- [ ] Logs written to journal table
- [ ] Market data API returns data
- [ ] Tools execute without errors

### API Endpoints
- [ ] GET /api/data returns latest scan
- [ ] GET /api/settings returns settings
- [ ] POST /api/settings updates settings
- [ ] GET /api/alerts returns alerts
- [ ] POST /api/alerts creates alert
- [ ] DELETE /api/alerts/<id> deletes alert
- [ ] GET /api/positions returns positions
- [ ] POST /api/positions creates position
- [ ] Similar tests for calls, earnings, routines

### Real-time Updates
- [ ] WebSocket connects (green status indicator)
- [ ] New scan triggers dashboard refresh
- [ ] Position update reflects in UI
- [ ] Alert trigger shows notification
- [ ] Journal entry appears in activity feed
- [ ] Browser notifications work (if permission granted)

### Scheduled Tasks
- [ ] Morning scan runs at 8:30 AM ET
- [ ] Market monitor runs every 15 mins during hours
- [ ] Tasks log to journal on completion
- [ ] Errors logged on failure

### Docker
- [ ] Image builds successfully
- [ ] Container starts and serves on :8080
- [ ] Health check passes
- [ ] Volume mounts work (code hot reload)
- [ ] Logs accessible via docker-compose logs

---

## Estimated Timeline

- **Supabase setup:** 15 minutes
- **Code changes:** 2-3 hours
  - utils.py rewrite: 30 min
  - routes.py updates: 45 min
  - Agent tools: 45 min
  - Frontend real-time: 30 min
  - Testing/debugging: 30 min
- **Docker setup:** 30 minutes
- **Testing:** 1 hour
- **Pi deployment:** 30 minutes

**Total: ~4-5 hours** for complete migration

---

## Rollback Plan

Since we're starting fresh (no data migration), rollback is simple:

1. Keep current code in `main` branch
2. Do migration work in `feature/supabase-migration` branch
3. If issues arise, switch back to `main` and restart service
4. No data loss risk (old JSON files preserved in git history)

---

## Future Enhancements

Post-migration improvements to consider:

1. **Row Level Security (RLS):** Add Supabase policies for multi-user access
2. **API Authentication:** Protect endpoints with API keys or JWT
3. **Metrics Dashboard:** Visualize scan history, position performance, win rate
4. **Backtesting:** Query historical scans to test strategy changes
5. **Multi-timeframe Scans:** Daily, weekly, monthly scans in separate tables
6. **Webhook Alerts:** Send alerts to Slack/Discord/Telegram
7. **Mobile App:** React Native app with Supabase real-time subscriptions

---

## Success Criteria

Migration is successful when:

âœ… Pi runs 24/7 without external dependencies (no gog CLI)
âœ… Wilson autonomously generates scans at 8:30 AM ET daily
âœ… All data persists in Supabase (no local JSON files)
âœ… Dashboard updates in real-time via WebSockets
âœ… Docker testing works locally without Pi access
âœ… Historical scans queryable via SQL
âœ… System recoverable from clean Pi install (stateless)

---

## Appendix: Key Files Changed

### Files to Modify
- `app/dashboard/utils.py` - Complete rewrite
- `app/dashboard/routes.py` - Update all endpoints
- `app/agents/tools.py` - Add 7 new tools
- `app/tasks.py` - Update morning_briefing prompt
- `app/templates/index.html` - Add Supabase JS client
- `app/extensions.py` - Add SUPABASE_ANON_KEY config
- `.env` - Update credentials
- `pyproject.toml` - Add polygon-api-client or finnhub-python

### Files to Create
- `Dockerfile` - Container build instructions
- `docker-compose.yml` - Local development stack
- `.dockerignore` - Build exclusions
- `docs/plans/2026-02-11-supabase-migration-design.md` - This document

### Files to Delete
- `app/data/` directory and all contents (after migration)
- Any cached JSON files

### Files Unchanged
- `run.py` - Entry point stays the same
- `app/__init__.py` - App factory unchanged
- `app/agents/wilson.py` - Agent definition unchanged (tools registered)
- `app/agents/prompts.py` - System prompts unchanged
- All template files except `index.html`

---

**End of Design Document**
