# Supabase Migration Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Migrate DeepDiver from Google Sheets + Local JSON to full Supabase cloud architecture with autonomous Wilson scanning.

**Architecture:** Replace Google Sheets data fetching (gog CLI) with Wilson agent autonomous scanning using market data APIs. Replace all local JSON file storage with Supabase PostgreSQL tables. Add real-time WebSocket updates to dashboard.

**Tech Stack:** Supabase (PostgreSQL + Realtime), polygon.io API, Flask, Supabase JS client

---

## Task 1: Update Environment and Dependencies

**Files:**
- Modify: `.env.example`
- Modify: `pyproject.toml`

**Step 1: Update .env.example with new credentials**

Remove Google Sheets config, add Supabase and market data API:

```bash
# Edit .env.example
```

Remove these lines:
```
GOOGLE_SHEET_ID=your_sheet_id_here
SHEET_RANGE='Main'!A1:W50
GOG_ACCOUNT=google-sheet@openclaw-gmail
CACHE_DURATION=300
```

Add these lines:
```
# --- Supabase Config (expanded) ---
SUPABASE_URL=your_supabase_url_here
SUPABASE_KEY=your_supabase_service_role_key_here
SUPABASE_ANON_KEY=your_supabase_anon_key_here

# --- Market Data API ---
POLYGON_API_KEY=your_polygon_api_key_here
```

**Step 2: Update your local .env file**

Copy .env.example to .env and fill in real credentials:
```bash
cp .env.example .env
nano .env  # Add your real Supabase and Polygon credentials
```

**Step 3: Add polygon-api-client dependency**

```bash
uv add polygon-api-client
```

Expected: "Added polygon-api-client to pyproject.toml"

**Step 4: Sync dependencies**

```bash
uv sync
```

Expected: Dependencies installed successfully

---

## Task 2: Create Supabase Database Schema

**Files:**
- Create: `docs/supabase-schema.sql`

**Step 1: Create schema SQL file**

```sql
-- DeepDiver Supabase Schema
-- Run this in Supabase SQL Editor

-- 1. CANSLIM Scans
CREATE TABLE scans (
  id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  created_at TIMESTAMPTZ DEFAULT NOW() NOT NULL,
  scan_time TIMESTAMPTZ NOT NULL,
  market_regime TEXT NOT NULL,
  dist_days TEXT,
  buy_ok TEXT,
  account_balance NUMERIC,
  risk_per_trade NUMERIC,
  actionable_count INT,
  metadata JSONB
);

-- 2. Stock Candidates
CREATE TABLE scan_stocks (
  id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  scan_id BIGINT REFERENCES scans(id) ON DELETE CASCADE,
  ticker TEXT NOT NULL,
  pivot NUMERIC,
  stop NUMERIC,
  rs_rating NUMERIC,
  comp_rating NUMERIC,
  eps_rating NUMERIC,
  setup_type TEXT,
  notes TEXT,
  metadata JSONB,
  created_at TIMESTAMPTZ DEFAULT NOW()
);
CREATE INDEX idx_scan_stocks_scan_id ON scan_stocks(scan_id);
CREATE INDEX idx_scan_stocks_ticker ON scan_stocks(ticker);

-- 3. Settings
CREATE TABLE settings (
  key TEXT PRIMARY KEY,
  value JSONB NOT NULL,
  updated_at TIMESTAMPTZ DEFAULT NOW()
);
INSERT INTO settings (key, value) VALUES
  ('account_equity', '100000'),
  ('risk_pct', '0.01'),
  ('max_positions', '6');

-- 4. Alerts
CREATE TABLE alerts (
  id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  ticker TEXT NOT NULL,
  condition TEXT NOT NULL,
  price NUMERIC NOT NULL,
  triggered BOOLEAN DEFAULT FALSE,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- 5. Earnings
CREATE TABLE earnings (
  ticker TEXT PRIMARY KEY,
  earnings_date DATE NOT NULL,
  updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- 6. Positions
CREATE TABLE positions (
  id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  ticker TEXT NOT NULL,
  account TEXT DEFAULT 'default',
  trade_type TEXT DEFAULT 'long',
  entry_date DATE NOT NULL,
  entry_price NUMERIC NOT NULL,
  shares INT NOT NULL,
  cost_basis NUMERIC NOT NULL,
  stop_price NUMERIC,
  target_price NUMERIC,
  setup_type TEXT,
  status TEXT DEFAULT 'open',
  close_date DATE,
  close_price NUMERIC,
  pnl NUMERIC,
  notes TEXT,
  created_at TIMESTAMPTZ DEFAULT NOW()
);
CREATE INDEX idx_positions_status ON positions(status);

-- 7. Covered Calls
CREATE TABLE covered_calls (
  id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  ticker TEXT NOT NULL,
  sell_date DATE NOT NULL,
  expiry DATE NOT NULL,
  strike NUMERIC NOT NULL,
  contracts INT NOT NULL,
  premium_per_contract NUMERIC NOT NULL,
  premium_total NUMERIC NOT NULL,
  delta NUMERIC,
  stock_price_at_sell NUMERIC,
  status TEXT DEFAULT 'open',
  close_date DATE,
  close_price NUMERIC,
  pnl NUMERIC,
  notes TEXT,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- 8. Routines
CREATE TABLE routines (
  id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  date DATE NOT NULL,
  routine_type TEXT NOT NULL,
  data JSONB NOT NULL,
  updated_at TIMESTAMPTZ DEFAULT NOW(),
  UNIQUE(date, routine_type)
);

-- 9. Watchlist (from project_vision.md)
CREATE TABLE watchlist (
  ticker TEXT PRIMARY KEY,
  status TEXT NOT NULL,
  sentiment_score NUMERIC,
  last_updated TIMESTAMPTZ DEFAULT NOW()
);

-- 10. Bot Config (from project_vision.md)
CREATE TABLE bot_config (
  key TEXT PRIMARY KEY,
  value TEXT,
  description TEXT
);
INSERT INTO bot_config (key, value, description)
VALUES ('trading_enabled', 'true', 'Master switch for trade execution');

-- Note: journal table should already exist from earlier setup
-- If not, create it:
CREATE TABLE IF NOT EXISTS journal (
  id BIGINT GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
  created_at TIMESTAMPTZ DEFAULT NOW() NOT NULL,
  agent_name TEXT NOT NULL,
  category TEXT NOT NULL,
  content TEXT NOT NULL,
  meta JSONB
);
```

**Step 2: Run schema in Supabase**

1. Go to your Supabase project ‚Üí SQL Editor
2. Create new query
3. Paste the schema SQL
4. Click "Run"
5. Verify: Check Database ‚Üí Tables to see all 11 tables

**Step 3: Enable Realtime replication**

1. Go to Database ‚Üí Replication
2. Enable replication for these tables:
   - scans
   - scan_stocks
   - positions
   - alerts
   - journal
3. Click "Save"

---

## Task 3: Rewrite app/dashboard/utils.py

**Files:**
- Modify: `app/dashboard/utils.py` (complete rewrite)

**Step 1: Backup and clear utils.py**

```bash
# Keep backup in git history, but clear file
# We'll rewrite from scratch
```

**Step 2: Write new utils.py with Supabase helpers**

Replace entire contents with:

```python
import os
from app.extensions import supabase

# Configuration
DATA_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'data')

# Keep DATA_DIR for backwards compatibility, but won't use it
os.makedirs(DATA_DIR, exist_ok=True)

DEFAULT_SETTINGS = {
    'account_equity': 100000,
    'risk_pct': 0.01,
    'max_positions': 6
}

# Scan helpers
def get_latest_scan():
    """Get most recent CANSLIM scan with stocks."""
    try:
        result = supabase.table('scans') \
            .select('*, scan_stocks(*)') \
            .order('created_at', desc=True) \
            .limit(1) \
            .execute()
        return result.data[0] if result.data else None
    except Exception as e:
        print(f"Error fetching latest scan: {e}")
        return None

def get_scan_by_id(scan_id):
    """Get specific scan with stocks."""
    try:
        result = supabase.table('scans') \
            .select('*, scan_stocks(*)') \
            .eq('id', scan_id) \
            .single() \
            .execute()
        return result.data
    except Exception as e:
        print(f"Error fetching scan {scan_id}: {e}")
        return None

def get_all_scans(limit=50):
    """Get historical scans."""
    try:
        result = supabase.table('scans') \
            .select('id, created_at, scan_time, market_regime, actionable_count') \
            .order('created_at', desc=True) \
            .limit(limit) \
            .execute()
        return result.data
    except Exception as e:
        print(f"Error fetching scans: {e}")
        return []

# Settings helpers
def get_settings():
    """Get all settings as dict."""
    try:
        result = supabase.table('settings').select('*').execute()
        settings = {row['key']: row['value'] for row in result.data}
        # Merge with defaults for missing keys
        for k, v in DEFAULT_SETTINGS.items():
            if k not in settings:
                settings[k] = v
        return settings
    except Exception as e:
        print(f"Error fetching settings: {e}")
        return DEFAULT_SETTINGS.copy()

def update_setting(key, value):
    """Update a single setting."""
    try:
        supabase.table('settings') \
            .upsert({'key': key, 'value': value}) \
            .execute()
        return True
    except Exception as e:
        print(f"Error updating setting {key}: {e}")
        return False

# Alert helpers
def get_all_alerts():
    """Get all alerts."""
    try:
        result = supabase.table('alerts') \
            .select('*') \
            .order('created_at', desc=True) \
            .execute()
        return result.data
    except Exception as e:
        print(f"Error fetching alerts: {e}")
        return []

def add_alert(ticker, condition, price):
    """Add new alert."""
    try:
        result = supabase.table('alerts').insert({
            'ticker': ticker.upper(),
            'condition': condition,
            'price': float(price),
            'triggered': False
        }).execute()
        return result.data[0] if result.data else None
    except Exception as e:
        print(f"Error adding alert: {e}")
        return None

def delete_alert(alert_id):
    """Delete alert by ID."""
    try:
        supabase.table('alerts').delete().eq('id', alert_id).execute()
        return True
    except Exception as e:
        print(f"Error deleting alert: {e}")
        return False

# Earnings helpers
def get_all_earnings():
    """Get earnings calendar."""
    try:
        result = supabase.table('earnings').select('*').execute()
        return {row['ticker']: row['earnings_date'] for row in result.data}
    except Exception as e:
        print(f"Error fetching earnings: {e}")
        return {}

def set_earnings_date(ticker, date):
    """Set earnings date for ticker."""
    try:
        supabase.table('earnings') \
            .upsert({'ticker': ticker.upper(), 'earnings_date': date}) \
            .execute()
        return True
    except Exception as e:
        print(f"Error setting earnings for {ticker}: {e}")
        return False

# Position helpers
def get_all_positions(status=None):
    """Get positions filtered by status."""
    try:
        query = supabase.table('positions').select('*')
        if status:
            query = query.eq('status', status)
        result = query.order('entry_date', desc=True).execute()
        return result.data
    except Exception as e:
        print(f"Error fetching positions: {e}")
        return []

def add_position(position_data):
    """Add new position."""
    try:
        result = supabase.table('positions').insert(position_data).execute()
        return result.data[0] if result.data else None
    except Exception as e:
        print(f"Error adding position: {e}")
        return None

def update_position(position_id, updates):
    """Update position."""
    try:
        result = supabase.table('positions') \
            .update(updates) \
            .eq('id', position_id) \
            .execute()
        return result.data[0] if result.data else None
    except Exception as e:
        print(f"Error updating position: {e}")
        return None

def delete_position(position_id):
    """Delete position."""
    try:
        supabase.table('positions').delete().eq('id', position_id).execute()
        return True
    except Exception as e:
        print(f"Error deleting position: {e}")
        return False

def _positions_summary(positions):
    """Calculate positions summary."""
    open_pos = [p for p in positions if p.get('status') == 'open']
    closed_pos = [p for p in positions if p.get('status') != 'open']
    total_pnl = sum(p.get('pnl', 0) or 0 for p in closed_pos)

    return {
        'open_count': len(open_pos),
        'closed_count': len(closed_pos),
        'total_pnl': total_pnl
    }

# Covered calls helpers
def get_all_calls():
    """Get all covered calls."""
    try:
        result = supabase.table('covered_calls') \
            .select('*') \
            .order('sell_date', desc=True) \
            .execute()
        return result.data
    except Exception as e:
        print(f"Error fetching calls: {e}")
        return []

def add_call(call_data):
    """Add new covered call."""
    try:
        result = supabase.table('covered_calls').insert(call_data).execute()
        return result.data[0] if result.data else None
    except Exception as e:
        print(f"Error adding call: {e}")
        return None

def update_call(call_id, updates):
    """Update covered call."""
    try:
        result = supabase.table('covered_calls') \
            .update(updates) \
            .eq('id', call_id) \
            .execute()
        return result.data[0] if result.data else None
    except Exception as e:
        print(f"Error updating call: {e}")
        return None

def delete_call(call_id):
    """Delete covered call."""
    try:
        supabase.table('covered_calls').delete().eq('id', call_id).execute()
        return True
    except Exception as e:
        print(f"Error deleting call: {e}")
        return False

def _calls_summary(trades):
    """Calculate calls summary."""
    def _summarize(subset, capital=100000):
        if not subset:
            return {'total_premium': 0, 'total_pnl': 0, 'total_trades': 0,
                    'expired': 0, 'called_away': 0, 'open': 0,
                    'weekly_avg': 0, 'annualized_yield': 0}
        total_premium = sum(t.get('premium_total', 0) for t in subset)
        closed = [t for t in subset if t.get('status') != 'open']
        total_pnl = sum(t.get('pnl', t.get('premium_total', 0)) for t in closed)
        open_t = [t for t in subset if t.get('status') == 'open']
        expired = [t for t in subset if t.get('status') == 'expired']
        called = [t for t in subset if t.get('status') == 'called_away']
        if subset:
            dates = sorted(set(t.get('sell_date', '')[:7] for t in subset if t.get('sell_date')))
            months = max(len(dates), 1)
            annualized = (total_premium / months) * 12 / max(capital, 1) * 100
        else:
            annualized = 0
        return {
            'total_premium': total_premium,
            'total_pnl': total_pnl,
            'total_trades': len(subset),
            'expired': len(expired),
            'called_away': len(called),
            'open': len(open_t),
            'weekly_avg': total_premium / max(len(subset), 1),
            'annualized_yield': annualized,
        }

    overall = _summarize(trades)
    tickers = sorted(set(t.get('ticker', 'SPY') for t in trades)) if trades else []
    by_ticker = {}
    for tk in tickers:
        subset = [t for t in trades if t.get('ticker', 'SPY') == tk]
        by_ticker[tk] = _summarize(subset, 100000)

    overall['tickers'] = tickers
    overall['by_ticker'] = by_ticker
    return overall

# Routine helpers
def get_routine(date_str):
    """Get routine for specific date."""
    try:
        result = supabase.table('routines') \
            .select('*') \
            .eq('date', date_str) \
            .execute()

        routines = {'date': date_str}
        for row in result.data:
            routines[row['routine_type']] = row['data']
        return routines
    except Exception as e:
        print(f"Error fetching routine for {date_str}: {e}")
        return {'date': date_str}

def save_routine(date_str, routine_type, data):
    """Save routine data."""
    try:
        supabase.table('routines') \
            .upsert({
                'date': date_str,
                'routine_type': routine_type,
                'data': data
            }) \
            .execute()
        return True
    except Exception as e:
        print(f"Error saving routine: {e}")
        return False

def get_all_routine_dates():
    """Get set of dates that have routine records."""
    try:
        result = supabase.table('routines') \
            .select('date, routine_type') \
            .execute()

        dates = {}
        for row in result.data:
            ds = row['date']
            if ds not in dates:
                dates[ds] = {'has_premarket': False, 'has_postclose': False}
            if row['routine_type'] == 'premarket':
                dates[ds]['has_premarket'] = True
            elif row['routine_type'] == 'postclose':
                dates[ds]['has_postclose'] = True
        return dates
    except Exception as e:
        print(f"Error fetching routine dates: {e}")
        return {}

# Backwards compatibility - legacy function names
load_calls = get_all_calls
save_calls = lambda trades: None  # No-op, use add_call/update_call instead
load_positions = lambda: get_all_positions()
save_positions = lambda positions: None  # No-op, use add_position/update_position instead
load_routine = get_routine
```

**Step 3: Verify Supabase connection**

Test the helpers work:
```bash
uv run python -c "
from app.dashboard.utils import get_settings
print(get_settings())
"
```

Expected output: `{'account_equity': 100000, 'risk_pct': 0.01, 'max_positions': 6}`

---

## Task 4: Update app/dashboard/routes.py

**Files:**
- Modify: `app/dashboard/routes.py`

**Step 1: Update imports**

Change line 8-14 from:
```python
from .utils import (
    get_cached_data, load_json_file, save_json_file,
    load_routine, save_routine, get_all_routine_dates,
    load_calls, save_calls, _calls_summary,
    load_positions, save_positions, _positions_summary,
    SETTINGS_FILE, ALERTS_FILE, EARNINGS_FILE, HISTORY_DIR,
    DEFAULT_SETTINGS, SECTION_DIVIDERS
)
```

To:
```python
from .utils import (
    get_latest_scan, get_scan_by_id, get_all_scans,
    get_settings, update_setting,
    get_all_alerts, add_alert, delete_alert,
    get_all_earnings, set_earnings_date,
    get_all_positions, add_position, update_position, delete_position, _positions_summary,
    get_all_calls, add_call, update_call, delete_call, _calls_summary,
    get_routine, save_routine, get_all_routine_dates,
    DEFAULT_SETTINGS
)
```

**Step 2: Update /api/data endpoint**

Replace lines 26-65 with:
```python
@bp.route('/api/data')
def api_data():
    """Return latest scan data as JSON"""
    try:
        scan = get_latest_scan()
        if scan is None:
            return jsonify({'error': 'No scans found'}), 404

        # Calculate Shares and Cost for each stock
        settings = get_settings()
        account_equity = float(settings.get('account_equity', 100000))
        risk_pct = float(settings.get('risk_pct', 0.01))
        risk_per_trade = account_equity * risk_pct

        for stock in scan.get('scan_stocks', []):
            try:
                pivot = float(stock.get('pivot', 0))
                stop = float(stock.get('stop', 0))
                if pivot > 0 and stop > 0 and pivot > stop:
                    risk_per_share = pivot - stop
                    shares = int(risk_per_trade / risk_per_share)
                    stock['Shares'] = str(shares)
                    stock['Cost'] = f"${shares * pivot:,.0f}"
                else:
                    stock['Shares'] = ''
                    stock['Cost'] = ''
            except (ValueError, ZeroDivisionError):
                stock['Shares'] = ''
                stock['Cost'] = ''

        return jsonify(scan)
    except Exception as e:
        return jsonify({'error': str(e)}), 500
```

**Step 3: Update /api/refresh endpoint**

Replace lines 67-73 with:
```python
@bp.route('/api/refresh')
def api_refresh():
    """Force refresh (no-op now, just returns latest data)"""
    return api_data()
```

**Step 4: Update /api/history endpoints**

Replace lines 213-250 with:
```python
@bp.route('/api/history', methods=['GET'])
def get_history():
    """List all historical scans"""
    try:
        scans = get_all_scans(limit=100)
        return jsonify(scans)
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@bp.route('/api/history/<int:scan_id>', methods=['GET'])
def get_historical_scan(scan_id):
    """Get specific historical scan"""
    try:
        scan = get_scan_by_id(scan_id)
        if not scan:
            return jsonify({'error': 'Scan not found'}), 404
        return jsonify(scan)
    except Exception as e:
        return jsonify({'error': str(e)}), 500
```

**Step 5: Update /api/settings endpoints**

Replace lines 252-282 with:
```python
@bp.route('/api/settings', methods=['GET'])
def get_settings_api():
    """Get scanner settings"""
    return jsonify(get_settings())

@bp.route('/api/settings', methods=['POST'])
def update_settings_api():
    """Update scanner settings"""
    try:
        data = request.json
        if 'account_equity' in data:
            update_setting('account_equity', float(data['account_equity']))
        if 'risk_pct' in data:
            update_setting('risk_pct', float(data['risk_pct']))
        if 'max_positions' in data:
            update_setting('max_positions', int(data['max_positions']))

        return jsonify(get_settings()), 200
    except Exception as e:
        return jsonify({'error': str(e)}), 500
```

**Step 6: Update /api/alerts endpoints**

Replace lines 114-181 with:
```python
@bp.route('/api/alerts', methods=['GET'])
def get_alerts():
    """Get all alerts"""
    return jsonify(get_all_alerts())

@bp.route('/api/alerts', methods=['POST'])
def add_alert_api():
    """Add a new alert"""
    try:
        data = request.json or {}

        # Validate ticker
        ticker = data.get('ticker', '').strip().upper()
        if not ticker or len(ticker) > 10 or not ticker.replace('.', '').replace('-', '').isalnum():
            return jsonify({'error': 'Invalid ticker (max 10 alphanumeric chars)'}), 400

        # Validate condition
        condition = data.get('condition', 'above')
        if condition not in ['above', 'below']:
            return jsonify({'error': 'Invalid condition (must be above or below)'}), 400

        # Validate price
        try:
            price = float(data.get('price', 0))
            if price <= 0 or price > 1000000:
                return jsonify({'error': 'Invalid price (must be positive, max $1M)'}), 400
        except (ValueError, TypeError):
            return jsonify({'error': 'Invalid price (must be a number)'}), 400

        alert = add_alert(ticker, condition, price)
        if alert:
            return jsonify(alert), 201
        else:
            return jsonify({'error': 'Failed to save alert'}), 500

    except Exception as e:
        return jsonify({'error': str(e)}), 500

@bp.route('/api/alerts/<int:alert_id>', methods=['DELETE'])
def delete_alert_api(alert_id):
    """Delete an alert by ID"""
    try:
        if delete_alert(alert_id):
            return jsonify({'ok': True}), 200
        else:
            return jsonify({'error': 'Failed to delete alert'}), 500
    except Exception as e:
        return jsonify({'error': str(e)}), 500
```

**Step 7: Update /api/earnings endpoints**

Replace lines 183-210 with:
```python
@bp.route('/api/earnings', methods=['GET'])
def get_earnings():
    """Get all earnings dates"""
    return jsonify(get_all_earnings())

@bp.route('/api/earnings', methods=['POST'])
def set_earnings():
    """Set earnings date for a ticker"""
    try:
        data = request.json
        ticker = data.get('ticker', '').upper()
        date = data.get('date', '')

        if not ticker or not date:
            return jsonify({'error': 'Invalid ticker or date'}), 400

        if set_earnings_date(ticker, date):
            return jsonify({'ticker': ticker, 'date': date}), 200
        else:
            return jsonify({'error': 'Failed to save earnings date'}), 500

    except Exception as e:
        return jsonify({'error': str(e)}), 500
```

**Step 8: Update /api/positions endpoints**

Replace lines 459-532 with:
```python
@bp.route('/api/positions', methods=['GET'])
def api_positions_get():
    positions = get_all_positions()
    return jsonify({'positions': positions, 'summary': _positions_summary(positions)})

@bp.route('/api/positions', methods=['POST'])
def api_positions_add():
    try:
        data = request.json or {}

        # Validate ticker
        ticker = data.get('ticker', '').strip().upper()
        if not ticker or len(ticker) > 10 or not ticker.replace('.', '').replace('-', '').isalnum():
            return jsonify({'error': 'Invalid ticker (max 10 alphanumeric chars)'}), 400

        # Validate shares
        try:
            shares = int(data.get('shares', 0))
            if shares <= 0 or shares > 1000000:
                return jsonify({'error': 'Invalid shares (must be 1-1,000,000)'}), 400
        except (ValueError, TypeError):
            return jsonify({'error': 'Invalid shares (must be an integer)'}), 400

        # Validate entry_price
        try:
            entry_price = float(data.get('entry_price', 0))
            if entry_price <= 0 or entry_price > 100000:
                return jsonify({'error': 'Invalid entry price (must be positive, max $100k)'}), 400
        except (ValueError, TypeError):
            return jsonify({'error': 'Invalid entry price (must be a number)'}), 400

        # Validate optional prices
        stop_price = float(data.get('stop_price', 0)) if data.get('stop_price') else 0
        target_price = float(data.get('target_price', 0)) if data.get('target_price') else 0

        # Validate trade_type
        trade_type = data.get('trade_type', 'long')
        if trade_type not in ['long', 'short']:
            return jsonify({'error': 'Invalid trade_type (must be long or short)'}), 400

        from datetime import datetime
        position = {
            'ticker': ticker,
            'account': data.get('account', 'default'),
            'trade_type': trade_type,
            'entry_date': data.get('entry_date', datetime.now().strftime('%Y-%m-%d')),
            'entry_price': entry_price,
            'shares': shares,
            'cost_basis': round(shares * entry_price, 2),
            'stop_price': stop_price,
            'target_price': target_price,
            'setup_type': data.get('setup_type', ''),
            'status': 'open',
            'close_date': None,
            'close_price': None,
            'pnl': None,
            'notes': data.get('notes', ''),
        }

        result = add_position(position)
        if result:
            return jsonify({'ok': True, 'position': result}), 201
        else:
            return jsonify({'error': 'Failed to add position'}), 500
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@bp.route('/api/positions/<int:position_id>', methods=['PATCH'])
def api_positions_update(position_id):
    try:
        data = request.json or {}
        result = update_position(position_id, data)
        if result:
            return jsonify({'ok': True, 'position': result})
        else:
            return jsonify({'error': 'Failed to update position'}), 500
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@bp.route('/api/positions/<int:position_id>', methods=['DELETE'])
def api_positions_delete(position_id):
    try:
        if delete_position(position_id):
            return jsonify({'ok': True})
        else:
            return jsonify({'error': 'Failed to delete position'}), 500
    except Exception as e:
        return jsonify({'error': str(e)}), 500
```

**Step 9: Update /api/calls endpoints**

Replace lines 358-451 with:
```python
@bp.route('/api/calls', methods=['GET'])
def api_calls_get():
    trades = get_all_calls()
    return jsonify({'trades': trades, 'summary': _calls_summary(trades)})

@bp.route('/api/calls', methods=['POST'])
def api_calls_add():
    try:
        data = request.json or {}

        # Validate ticker
        ticker = data.get('ticker', 'SPY').strip().upper()
        if not ticker or len(ticker) > 10 or not ticker.replace('.', '').replace('-', '').isalnum():
            return jsonify({'error': 'Invalid ticker (max 10 alphanumeric chars)'}), 400

        # Validate contracts
        try:
            contracts = int(data.get('contracts', 1))
            if contracts <= 0 or contracts > 10000:
                return jsonify({'error': 'Invalid contracts (must be 1-10,000)'}), 400
        except (ValueError, TypeError):
            return jsonify({'error': 'Invalid contracts (must be an integer)'}), 400

        # Validate premium_per_contract
        try:
            premium_per = float(data.get('premium_per_contract', 0))
            if premium_per < 0 or premium_per > 10000:
                return jsonify({'error': 'Invalid premium (must be 0-$10,000)'}), 400
        except (ValueError, TypeError):
            return jsonify({'error': 'Invalid premium (must be a number)'}), 400

        # Validate strike
        try:
            strike = float(data.get('strike', 0))
            if strike <= 0 or strike > 100000:
                return jsonify({'error': 'Invalid strike (must be positive, max $100k)'}), 400
        except (ValueError, TypeError):
            return jsonify({'error': 'Invalid strike (must be a number)'}), 400

        from datetime import datetime
        trade = {
            'ticker': ticker,
            'sell_date': data.get('sell_date', datetime.now().strftime('%Y-%m-%d')),
            'expiry': data.get('expiry', ''),
            'strike': strike,
            'contracts': contracts,
            'premium_per_contract': premium_per,
            'premium_total': round(premium_per * contracts * 100, 2),
            'delta': data.get('delta', 0.10),
            'stock_price_at_sell': data.get('stock_price', 0),
            'status': 'open',
            'close_date': None,
            'close_price': None,
            'pnl': None,
            'notes': data.get('notes', ''),
        }

        result = add_call(trade)
        if result:
            return jsonify({'ok': True, 'trade': result}), 201
        else:
            return jsonify({'error': 'Failed to add call'}), 500
    except Exception as e:
        return jsonify({'error': f'Server error: {str(e)}'}), 500

@bp.route('/api/calls/<int:call_id>', methods=['PATCH'])
def api_calls_close(call_id):
    try:
        data = request.json
        calls = get_all_calls()

        # Find the call to update
        trade = None
        for t in calls:
            if t.get('id') == call_id:
                trade = t
                break

        if not trade:
            return jsonify({'error': 'Call not found'}), 404

        status = data.get('status', 'expired')
        updates = {
            'status': status,
            'close_date': data.get('close_date', datetime.now().strftime('%Y-%m-%d'))
        }

        if status == 'expired':
            updates['pnl'] = trade['premium_total']
        elif status == 'called_away':
            price_at_sell = trade.get('stock_price_at_sell', 0)
            appreciation = (trade['strike'] - price_at_sell) * trade['contracts'] * 100
            updates['pnl'] = round(trade['premium_total'] + appreciation, 2)
        else:
            buyback = data.get('buyback_price', 0) * trade['contracts'] * 100
            updates['pnl'] = round(trade['premium_total'] - buyback, 2)
            updates['close_price'] = data.get('buyback_price', 0)

        updates['notes'] = data.get('notes', trade.get('notes', ''))

        result = update_call(call_id, updates)
        if result:
            return jsonify({'ok': True})
        else:
            return jsonify({'error': 'Failed to update call'}), 500
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@bp.route('/api/calls/<int:call_id>', methods=['DELETE'])
def api_calls_delete(call_id):
    try:
        if delete_call(call_id):
            return jsonify({'ok': True})
        else:
            return jsonify({'error': 'Failed to delete call'}), 500
    except Exception as e:
        return jsonify({'error': str(e)}), 500
```

**Step 10: Update routine endpoints**

Replace lines 296-350 with:
```python
@bp.route('/routine/<date_str>/<routine_type>', methods=['GET', 'POST'])
def routine_form(date_str, routine_type):
    if routine_type not in ('premarket', 'postclose'):
        return 'Invalid type', 404

    data = get_routine(date_str)

    if request.method == 'POST':
        fields = {}
        for key in request.form:
            if key.startswith('routine_'):
                fields[key[8:]] = request.form[key]
        save_routine(date_str, routine_type, fields)
        return redirect(url_for('dashboard.routine_view', date_str=date_str))

    existing = data.get(routine_type, {})
    return render_template('routine_form.html', date_str=date_str,
                         routine_type=routine_type, data=existing)

@bp.route('/api/routine/<date_str>', methods=['GET'])
def api_routine_get(date_str):
    return jsonify(get_routine(date_str))

@bp.route('/api/routine/<date_str>', methods=['POST'])
def api_routine_save(date_str):
    req = request.json
    rtype = req.get('type', 'premarket')
    if rtype in ('premarket', 'postclose'):
        data = req.get('data', {})
        save_routine(date_str, rtype, data)
    return jsonify({'ok': True})
```

**Step 11: Test API endpoints**

```bash
# Start server
./run.sh

# In another terminal, test endpoints:
curl http://localhost:8080/api/settings
curl http://localhost:8080/api/data
curl http://localhost:8080/api/alerts
```

Expected: All return data from Supabase (or empty arrays if no data yet)

---

## Task 5: Add Wilson Agent Market Data Tools

**Files:**
- Modify: `app/agents/tools.py`

**Step 1: Add polygon.io integration**

Add to the top of `app/agents/tools.py`:

```python
import os
from polygon import RESTClient
```

**Step 2: Add fetch_market_data tool**

Add after existing imports:

```python
@tool
def fetch_market_data(tickers: str) -> str:
    """Fetches real-time price and technical data from Polygon.io.

    Args:
        tickers: Comma-separated list of stock symbols (e.g., "AAPL,MSFT,GOOGL")

    Returns:
        JSON string with price data and technical indicators for each ticker
    """
    api_key = os.getenv('POLYGON_API_KEY')
    if not api_key:
        return "Error: POLYGON_API_KEY not set in environment"

    try:
        client = RESTClient(api_key)
        ticker_list = [t.strip().upper() for t in tickers.split(',')]
        results = []

        for ticker in ticker_list:
            try:
                # Get latest price
                snapshot = client.get_snapshot_ticker('stocks', ticker)

                if snapshot and snapshot.ticker:
                    data = {
                        'ticker': ticker,
                        'price': snapshot.day.close if snapshot.day else 0,
                        'volume': snapshot.day.volume if snapshot.day else 0,
                        'change_pct': snapshot.day.change_percent if snapshot.day else 0,
                        'high': snapshot.day.high if snapshot.day else 0,
                        'low': snapshot.day.low if snapshot.day else 0,
                    }
                    results.append(data)
            except Exception as e:
                print(f"Error fetching {ticker}: {e}")
                continue

        import json
        return json.dumps(results, indent=2)

    except Exception as e:
        return f"Error fetching market data: {str(e)}"
```

**Step 3: Add write_scan_results tool**

Add after fetch_market_data:

```python
@tool
def write_scan_results(market_regime: str, stocks_json: str, metadata_json: str = "{}") -> str:
    """Writes CANSLIM scan results to Supabase.

    Args:
        market_regime: Current market condition (e.g., "Confirmed", "Rally Attempt")
        stocks_json: JSON string with list of stock candidates
        metadata_json: JSON string with scan metadata (optional)

    Returns:
        Confirmation message with scan ID
    """
    if not supabase:
        return "Error: Supabase not connected"

    try:
        import json
        from datetime import datetime

        stocks = json.loads(stocks_json)
        metadata = json.loads(metadata_json)

        # Create scan record
        scan_data = {
            'scan_time': datetime.now().isoformat(),
            'market_regime': market_regime,
            'dist_days': metadata.get('dist_days', ''),
            'buy_ok': metadata.get('buy_ok', ''),
            'account_balance': metadata.get('account_balance'),
            'risk_per_trade': metadata.get('risk_per_trade'),
            'actionable_count': len(stocks),
            'metadata': metadata
        }

        scan_result = supabase.table('scans').insert(scan_data).execute()
        scan_id = scan_result.data[0]['id']

        # Insert stocks
        for stock in stocks:
            stock['scan_id'] = scan_id

        supabase.table('scan_stocks').insert(stocks).execute()

        return f"‚úì Scan saved successfully (ID: {scan_id}, {len(stocks)} stocks)"

    except Exception as e:
        return f"Error saving scan: {str(e)}"
```

**Step 4: Add get_current_positions tool**

Add after write_scan_results:

```python
@tool
def get_current_positions() -> str:
    """Retrieves all open positions from Supabase.

    Returns:
        JSON string with list of open positions
    """
    if not supabase:
        return "Error: Supabase not connected"

    try:
        result = supabase.table('positions') \
            .select('*') \
            .eq('status', 'open') \
            .execute()

        import json
        return json.dumps(result.data, indent=2)

    except Exception as e:
        return f"Error fetching positions: {str(e)}"
```

**Step 5: Add get_watchlist tool**

Add after get_current_positions:

```python
@tool
def get_watchlist() -> str:
    """Gets stocks currently being monitored.

    Returns:
        JSON string with watchlist stocks
    """
    if not supabase:
        return "Error: Supabase not connected"

    try:
        result = supabase.table('watchlist').select('*').execute()

        import json
        return json.dumps(result.data, indent=2)

    except Exception as e:
        return f"Error fetching watchlist: {str(e)}"
```

**Step 6: Add update_position tool**

Add after get_watchlist:

```python
@tool
def update_position(position_id: int, updates_json: str) -> str:
    """Updates a position (change stop, close, etc.).

    Args:
        position_id: Position ID to update
        updates_json: JSON string with fields to update

    Returns:
        Confirmation message
    """
    if not supabase:
        return "Error: Supabase not connected"

    try:
        import json
        updates = json.loads(updates_json)

        result = supabase.table('positions') \
            .update(updates) \
            .eq('id', position_id) \
            .execute()

        return f"‚úì Position {position_id} updated successfully"

    except Exception as e:
        return f"Error updating position: {str(e)}"
```

**Step 7: Add check_alerts tool**

Add after update_position:

```python
@tool
def check_alerts() -> str:
    """Checks if any price alerts have triggered.

    Returns:
        JSON string with triggered alerts
    """
    if not supabase:
        return "Error: Supabase not connected"

    try:
        # Get all untriggered alerts
        result = supabase.table('alerts') \
            .select('*') \
            .eq('triggered', False) \
            .execute()

        alerts = result.data
        triggered = []

        # Check each alert (simplified - in production, fetch real prices)
        for alert in alerts:
            # TODO: Fetch current price from market data API
            # For now, just return the alerts
            pass

        import json
        return json.dumps(alerts, indent=2)

    except Exception as e:
        return f"Error checking alerts: {str(e)}"
```

**Step 8: Add add_to_watchlist tool**

Add after check_alerts:

```python
@tool
def add_to_watchlist(ticker: str, status: str, score: float = 50.0) -> str:
    """Adds or updates a stock in the watchlist.

    Args:
        ticker: Stock symbol
        status: 'Watching', 'Long', or 'Short'
        score: Sentiment score 0-100

    Returns:
        Confirmation message
    """
    if not supabase:
        return "Error: Supabase not connected"

    try:
        data = {
            'ticker': ticker.upper(),
            'status': status,
            'sentiment_score': score
        }

        supabase.table('watchlist').upsert(data).execute()

        return f"‚úì Added {ticker} to watchlist (status: {status})"

    except Exception as e:
        return f"Error adding to watchlist: {str(e)}"
```

**Step 9: Test tools**

```bash
uv run python -c "
from app.agents.tools import fetch_market_data
result = fetch_market_data('AAPL')
print(result)
"
```

Expected: JSON with AAPL price data from Polygon

---

## Task 6: Update Wilson Agent Configuration

**Files:**
- Modify: `app/agents/wilson.py`

**Step 1: Register new tools**

Update the imports at top of file:

```python
from app.agents.tools import (
    log_journal,
    check_market_status,
    fetch_market_data,
    write_scan_results,
    get_current_positions,
    get_watchlist,
    update_position,
    check_alerts,
    add_to_watchlist
)
```

**Step 2: Add tools to Wilson agent**

Update the tools list in the Wilson agent definition:

```python
wilson = Agent(
    name="Wilson",
    model=model,
    intro=WILSON_SYSTEM_PROMPT,
    tools=[
        log_journal,
        check_market_status,
        fetch_market_data,
        write_scan_results,
        get_current_positions,
        get_watchlist,
        update_position,
        check_alerts,
        add_to_watchlist
    ]
)
```

**Step 3: Test Wilson with new tools**

```bash
uv run python -c "
from app.agents.wilson import wilson
response = wilson.run('Check the market status and fetch data for AAPL')
print(response.text)
"
```

Expected: Wilson uses check_market_status and fetch_market_data tools

---

## Task 7: Update Scheduled Tasks

**Files:**
- Modify: `app/tasks.py`

**Step 1: Update morning_briefing task**

Replace the task_morning_briefing function:

```python
@scheduler.task('cron', id='morning_briefing', day_of_week='mon-fri', hour=8, minute=30)
def task_morning_briefing():
    """Runs at 8:30 AM ET on Weekdays - Wilson performs CANSLIM scan."""
    print("‚è∞ Trigger: Morning Briefing / CANSLIM Scan")

    prompt = """It's 8:30 AM ET. Perform your morning CANSLIM scan:

    1. Check if market is open using check_market_status()
    2. Fetch market data for top momentum stocks using fetch_market_data()
       - Start with these tickers: AAPL,MSFT,GOOGL,NVDA,TSLA,META,AMZN
    3. Analyze each stock against CANSLIM criteria:
       - C: Current quarterly earnings (25%+ growth)
       - A: Annual earnings (25%+ growth over 3 years)
       - N: New highs, products, management
       - S: Supply & demand (volume, institutional buying)
       - L: Leader or laggard (RS rating > 80)
       - I: Institutional sponsorship
       - M: Market direction (confirm market regime)
    4. Determine market regime (Confirmed/Rally Attempt/Under Pressure/Correction)
    5. Write results using write_scan_results()
    6. Check alerts with check_alerts()
    7. Review current positions with get_current_positions()
    8. Log your analysis and recommendations to journal using log_journal()
    """

    try:
        response = wilson.run(prompt)
        print(f"‚úì Morning scan completed")
        print(f"Wilson's response: {response.text}")
    except Exception as e:
        print(f"‚úó Morning scan failed: {e}")
        # Log error to journal
        try:
            from app.agents.tools import log_journal
            log_journal("System", "Error", f"Morning scan failed: {e}")
        except:
            pass
```

**Step 2: Update market_monitor task**

Replace the task_market_monitor function:

```python
@scheduler.task('cron', id='market_monitor', day_of_week='mon-fri', hour='9-16', minute='*/15')
def task_market_monitor():
    """Runs every 15 mins during market hours - Wilson monitors positions."""
    print("‚è∞ Trigger: Market Monitor")

    prompt = """Monitor the market and current positions:

    1. Check current positions using get_current_positions()
    2. Check if any alerts have triggered using check_alerts()
    3. Fetch latest prices for positions using fetch_market_data()
    4. If any positions need attention (hit stops, reached targets), log to journal
    5. Update watchlist if you see new opportunities
    """

    try:
        response = wilson.run(prompt)
        print(f"‚úì Market monitor completed: {response.text}")
    except Exception as e:
        print(f"‚úó Market monitor failed: {e}")
```

**Step 3: Test scheduled tasks**

```bash
# Check scheduled jobs are registered
uv run python -c "
from app.extensions import scheduler
from app import create_app

app = create_app()
with app.app_context():
    jobs = scheduler.get_jobs()
    for job in jobs:
        print(f'Job: {job.id}, Next run: {job.next_run_time}')
"
```

Expected: Shows morning_briefing and market_monitor jobs

---

## Task 8: Add Real-time Frontend

**Files:**
- Modify: `app/templates/index.html`
- Modify: `app/extensions.py`

**Step 1: Expose SUPABASE_ANON_KEY to templates**

Add to `app/extensions.py` after init_supabase function:

```python
def get_supabase_config():
    """Get Supabase config for frontend."""
    return {
        'url': os.getenv('SUPABASE_URL'),
        'anon_key': os.getenv('SUPABASE_ANON_KEY')
    }
```

**Step 2: Update app factory to expose config**

In `app/__init__.py`, add before returning app:

```python
from app.extensions import get_supabase_config

@app.context_processor
def inject_supabase_config():
    return {'supabase_config': get_supabase_config()}
```

**Step 3: Add Supabase JS client to index.html**

In `app/templates/index.html`, add before closing `</head>` tag:

```html
<!-- Supabase JS Client -->
<script src="https://cdn.jsdelivr.net/npm/@supabase/supabase-js@2"></script>
```

**Step 4: Add real-time subscription code**

In `app/templates/index.html`, add before closing `</body>` tag:

```html
<script>
// Initialize Supabase client
const SUPABASE_CONFIG = {
    url: '{{ supabase_config.url }}',
    anonKey: '{{ supabase_config.anon_key }}'
};

let supabaseClient = null;

if (SUPABASE_CONFIG.url && SUPABASE_CONFIG.anonKey) {
    supabaseClient = window.supabase.createClient(
        SUPABASE_CONFIG.url,
        SUPABASE_CONFIG.anonKey
    );

    setupRealtimeSubscriptions();
}

function setupRealtimeSubscriptions() {
    if (!supabaseClient) return;

    // 1. Listen for new scans
    supabaseClient
        .channel('scans-channel')
        .on('postgres_changes',
            { event: 'INSERT', schema: 'public', table: 'scans' },
            (payload) => {
                console.log('üîç New scan detected:', payload.new);
                showNotification('New CANSLIM scan available');
                refreshData();
            })
        .subscribe();

    // 2. Listen for position changes
    supabaseClient
        .channel('positions-channel')
        .on('postgres_changes',
            { event: '*', schema: 'public', table: 'positions' },
            (payload) => {
                console.log('üìä Position updated:', payload);
                refreshData();
            })
        .subscribe();

    // 3. Listen for triggered alerts
    supabaseClient
        .channel('alerts-channel')
        .on('postgres_changes',
            { event: 'UPDATE', schema: 'public', table: 'alerts' },
            (payload) => {
                if (payload.new.triggered && !payload.old.triggered) {
                    const msg = `üö® Alert: ${payload.new.ticker} ${payload.new.condition} $${payload.new.price}`;
                    showNotification(msg);
                    playAlertSound();
                }
            })
        .subscribe();

    // 4. Listen for Wilson's journal entries
    supabaseClient
        .channel('journal-channel')
        .on('postgres_changes',
            { event: 'INSERT', schema: 'public', table: 'journal' },
            (payload) => {
                console.log('üìù Wilson logged:', payload.new);
                if (payload.new.category === 'Trade') {
                    showNotification(`Wilson: ${payload.new.content}`);
                }
            })
        .subscribe();

    // Connection status monitoring
    supabaseClient
        .channel('system')
        .on('system', { event: 'connected' }, () => {
            console.log('‚úì Real-time connected');
            updateConnectionStatus(true);
        })
        .on('system', { event: 'disconnected' }, () => {
            console.warn('‚úó Real-time disconnected');
            updateConnectionStatus(false);
        })
        .subscribe();
}

function showNotification(message) {
    // Browser notification
    if ('Notification' in window && Notification.permission === 'granted') {
        new Notification('DeepDiver', { body: message });
    }

    // In-app toast
    const toast = document.createElement('div');
    toast.className = 'toast-notification';
    toast.textContent = message;
    toast.style.cssText = `
        position: fixed;
        bottom: 20px;
        right: 20px;
        background: #333;
        color: white;
        padding: 15px 20px;
        border-radius: 8px;
        box-shadow: 0 4px 12px rgba(0,0,0,0.3);
        z-index: 1001;
        animation: slideIn 0.3s ease;
    `;
    document.body.appendChild(toast);
    setTimeout(() => toast.remove(), 5000);
}

function playAlertSound() {
    // Create simple beep with Web Audio API
    try {
        const audioContext = new (window.AudioContext || window.webkitAudioContext)();
        const oscillator = audioContext.createOscillator();
        const gainNode = audioContext.createGain();

        oscillator.connect(gainNode);
        gainNode.connect(audioContext.destination);

        oscillator.frequency.value = 800;
        oscillator.type = 'sine';

        gainNode.gain.setValueAtTime(0.3, audioContext.currentTime);
        gainNode.gain.exponentialRampToValueAtTime(0.01, audioContext.currentTime + 0.5);

        oscillator.start(audioContext.currentTime);
        oscillator.stop(audioContext.currentTime + 0.5);
    } catch (e) {
        console.log('Audio play failed:', e);
    }
}

function updateConnectionStatus(connected) {
    let indicator = document.getElementById('realtime-status');
    if (!indicator) {
        indicator = document.createElement('div');
        indicator.id = 'realtime-status';
        indicator.title = 'Real-time connection status';
        indicator.style.cssText = `
            position: fixed;
            top: 10px;
            right: 10px;
            width: 12px;
            height: 12px;
            border-radius: 50%;
            z-index: 1000;
        `;
        document.body.appendChild(indicator);
    }

    if (connected) {
        indicator.style.background = '#00ff00';
        indicator.style.boxShadow = '0 0 10px #00ff00';
    } else {
        indicator.style.background = '#ff0000';
        indicator.style.boxShadow = '0 0 10px #ff0000';
    }
}

// Request notification permission on load
if ('Notification' in window && Notification.permission === 'default') {
    Notification.requestPermission();
}

// Add CSS animation
const style = document.createElement('style');
style.textContent = `
@keyframes slideIn {
    from { transform: translateX(400px); opacity: 0; }
    to { transform: translateX(0); opacity: 1; }
}
`;
document.head.appendChild(style);
</script>
```

**Step 5: Test real-time updates**

1. Start the server: `./run.sh`
2. Open http://localhost:8080 in browser
3. Check console for "‚úì Real-time connected"
4. In Supabase SQL editor, insert a test scan:
```sql
INSERT INTO scans (scan_time, market_regime, actionable_count)
VALUES (NOW(), 'Confirmed', 5);
```
5. Browser should show notification "New CANSLIM scan available"

---

## Task 9: Create Docker Configuration

**Files:**
- Create: `Dockerfile`
- Create: `docker-compose.yml`
- Create: `.dockerignore`

**Step 1: Create Dockerfile**

```dockerfile
FROM python:3.11-slim

# Set working directory
WORKDIR /app

# Install uv
RUN pip install uv

# Copy dependency files
COPY pyproject.toml uv.lock ./

# Install dependencies
RUN uv sync --frozen

# Copy application code
COPY . .

# Expose port
EXPOSE 8080

# Run the application
CMD ["uv", "run", "run.py"]
```

**Step 2: Create docker-compose.yml**

```yaml
version: '3.8'

services:
  deepdiver:
    build: .
    container_name: deepdiver
    ports:
      - "8080:8080"
    env_file:
      - .env
    environment:
      - FLASK_ENV=development
      - PORT=8080
    volumes:
      # Mount code for hot reload during development
      - ./app:/app/app
      - ./run.py:/app/run.py
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
```

**Step 3: Create .dockerignore**

```
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
env/
venv/
.venv/
app/data/
*.json
.git/
.gitignore
.vscode/
.idea/
*.swp
.env.local
.env.production
*.md
docs/
*.log
```

**Step 4: Test Docker build**

```bash
docker-compose build
```

Expected: Image builds successfully

**Step 5: Test Docker run**

```bash
docker-compose up -d
docker-compose logs -f deepdiver
```

Expected: App starts on port 8080

**Step 6: Test Docker health**

```bash
curl http://localhost:8080/api/health
```

Expected: `{"status":"ok","app":"canslim-dashboard"}`

**Step 7: Test Wilson in Docker**

```bash
docker-compose exec deepdiver uv run python -c "
from app.agents.wilson import wilson
response = wilson.run('What time is it? Check market status.')
print(response.text)
"
```

Expected: Wilson responds with market status

---

## Task 10: Update Documentation

**Files:**
- Modify: `CLAUDE.md`
- Modify: `README.md` (if exists)

**Step 1: Update CLAUDE.md architecture section**

Update the "Data Flow" section (around line 122) to:

```markdown
### Data Flow

**Wilson ‚Üí Supabase ‚Üí Dashboard:**
1. Wilson uses `fetch_market_data()` to get real-time stock data
2. Wilson analyzes CANSLIM criteria autonomously
3. Wilson calls `write_scan_results()` to save to Supabase
4. Dashboard subscribes to Supabase Realtime
5. UI auto-updates when new scan inserted

**No More:**
- ~~Google Sheets integration (removed)~~
- ~~gog CLI dependency (removed)~~
- ~~Local JSON files for data persistence (removed)~~
- ~~Cache management (removed)~~
```

**Step 2: Update CLAUDE.md configuration section**

Remove Google Sheets section (lines 71-75), update to:

```markdown
**Market Data API:**
- `POLYGON_API_KEY` - Polygon.io API key for market data

**Supabase (Cloud Database):**
- `SUPABASE_URL` - Supabase project URL
- `SUPABASE_KEY` - Service role key (backend)
- `SUPABASE_ANON_KEY` - Anonymous key (frontend)

**AI Agent (Wilson):**
- `OPENROUTER_API_KEY` - OpenRouter API key for LLM access
- `GEMINI_API_KEY` - (Optional) Direct Gemini API fallback

**App:**
- `PORT` - Server port (default: 8080)
- `FLASK_ENV` - Environment (development/production)
```

**Step 3: Update troubleshooting section**

Replace "Failed to fetch data" section (around line 352) with:

```markdown
**"Failed to fetch data":**
- Check `SUPABASE_URL` and `SUPABASE_KEY` in `.env`
- Verify Supabase tables exist (run schema SQL)
- Test: `uv run python -c "from app.dashboard.utils import get_settings; print(get_settings())"`

**"No scans found":**
- Wilson must generate first scan via `task_morning_briefing`
- Manual trigger: `uv run python -c "from app.agents.wilson import wilson; wilson.run('Generate test scan')"`
- Check Supabase `scans` table has data

**Real-time not working:**
- Verify Realtime enabled in Supabase ‚Üí Database ‚Üí Replication
- Check browser console for WebSocket connection
- Verify `SUPABASE_ANON_KEY` is set correctly
```

---

## Task 11: Final Testing

**Files:**
- N/A (testing only)

**Step 1: Start fresh Docker environment**

```bash
docker-compose down
docker-compose build
docker-compose up -d
```

**Step 2: Verify all API endpoints**

```bash
# Settings
curl http://localhost:8080/api/settings

# Data (should return 404 if no scans yet)
curl http://localhost:8080/api/data

# Add alert
curl -X POST http://localhost:8080/api/alerts \
  -H "Content-Type: application/json" \
  -d '{"ticker":"AAPL","condition":"above","price":150}'

# Get alerts
curl http://localhost:8080/api/alerts

# Add position
curl -X POST http://localhost:8080/api/positions \
  -H "Content-Type: application/json" \
  -d '{"ticker":"MSFT","shares":100,"entry_price":350,"entry_date":"2026-02-11"}'

# Get positions
curl http://localhost:8080/api/positions
```

**Step 3: Test Wilson scan generation**

```bash
docker-compose exec deepdiver uv run python -c "
from app.agents.wilson import wilson

prompt = '''Generate a test CANSLIM scan:
1. Fetch data for AAPL, MSFT, GOOGL
2. Create a scan with market regime: Confirmed
3. Write results to Supabase
'''

response = wilson.run(prompt)
print(response.text)
"
```

**Step 4: Verify scan in Supabase**

1. Go to Supabase dashboard ‚Üí Table Editor
2. Check `scans` table has new row
3. Check `scan_stocks` table has stock records
4. Check `journal` table has Wilson's logs

**Step 5: Test real-time dashboard**

1. Open http://localhost:8080 in browser
2. Check green dot (top-right) showing real-time connected
3. In Supabase SQL editor, insert test scan:
```sql
INSERT INTO scans (scan_time, market_regime, actionable_count)
VALUES (NOW(), 'Rally Attempt', 3);
```
4. Browser should show notification instantly

**Step 6: Test scheduled tasks**

```bash
# Check scheduled jobs
docker-compose exec deepdiver uv run python -c "
from app.extensions import scheduler
from app import create_app

app = create_app()
with app.app_context():
    jobs = scheduler.get_jobs()
    for job in jobs:
        print(f'{job.id}: Next run at {job.next_run_time}')
"
```

Expected: Shows morning_briefing and market_monitor jobs with next run times

**Step 7: Clean up test data**

In Supabase SQL editor:
```sql
DELETE FROM scan_stocks WHERE scan_id IN (SELECT id FROM scans);
DELETE FROM scans;
DELETE FROM alerts;
DELETE FROM positions;
```

---

## Success Criteria

Migration is complete when:

‚úÖ Docker builds and runs successfully
‚úÖ All API endpoints return data from Supabase (not JSON files)
‚úÖ Wilson can generate scans using `fetch_market_data()` and `write_scan_results()`
‚úÖ Dashboard shows real-time updates (green dot, notifications work)
‚úÖ Scheduled tasks are registered and ready to run
‚úÖ No dependencies on Google Sheets or gog CLI
‚úÖ `app/data/` directory is empty/unused
‚úÖ All tests pass

---

## Rollback Instructions

If migration fails and you need to rollback:

1. Stop Docker: `docker-compose down`
2. Checkout previous code: `git checkout main` (or previous branch)
3. Restore old .env with Google Sheets credentials
4. Start app: `./run.sh`
5. Old system should work as before

---

**End of Implementation Plan**
