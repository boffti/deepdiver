# Project DeepDiver: Autonomous Trading Swarm Specification

**Version:** 1.0  
**Target Hardware:** Raspberry Pi 5 (8GB RAM + NVMe SSD recommended)  
**Architecture:** Monolithic Modular (Flask + APScheduler + Google ADK)  

---

## 1. System Overview

DeepDiver is an autonomous trading appliance designed to run 24/7 on a Raspberry Pi. It uses a "Swarm" of AI agents (orchestrated by a root agent named **Wilson**) to analyze market data, formulate strategies, and execute trades.

### Core Philosophy
* **Always On:** The system runs as a single Flask process managed by `systemd`.
* **Event-Driven:** Market events (Time, Webhooks) trigger Agent workflows via `APScheduler`.
* **Cloud State:** All persistence (Logs, Trade History) is offloaded to **Supabase** to save the Pi's SD card/SSD and enable real-time remote monitoring.
* **Pythonic Orchestration:** Scheduling is handled internally, keeping logic contained within the application code rather than scattered across OS cron jobs.

### High-Level Architecture

1.  **Orchestrator (Wilson):** The root Google ADK agent that receives tasks and delegates to tools.
2.  **Scheduler:** Triggers Wilson at specific times (e.g., 8:30 AM Market Prep).
3.  **Database:** Stores all "Memories" and "Logs" in Supabase (PostgreSQL).
4.  **Dashboard:** A lightweight Flask route that confirms system health.

---

## 2. Technology Stack

| Component | Technology | Reasoning |
| :--- | :--- | :--- |
| **Runtime** | Python 3.11+ | Native Google ADK support |
| **Web Framework** | Flask | Lightweight, extensible web server |
| **Scheduler** | Flask-APScheduler | Native Python scheduling inside the app context |
| **Intelligence** | **Google ADK** (Agent Dev Kit) | Structured agent definition and tool calling |
| **Database** | **Supabase** (PostgreSQL) | Free tier, Realtime websockets, remote access |
| **Model** | Gemini 2.0 Pro / Flash | Pro for reasoning, Flash for speed/cost |

---

## 3. Directory Structure

We use the **Application Factory Pattern** to ensure the scheduler and web server initialize cleanly together.

```text
/home/pi/deepdiver/
├── .env                    # Secrets (API Keys, URLs)
├── .gitignore
├── run.py                  # Application Entry Point
├── requirements.txt        # Dependencies
└── app/
    ├── __init__.py         # App Factory
    ├── extensions.py       # Shared instances (Supabase, Scheduler)
    ├── routes.py           # Dashboard Endpoints
    ├── tasks.py            # Scheduler Job Definitions
    └── agents/
        ├── __init__.py
        ├── wilson.py       # Main Orchestrator Definition
        ├── tools.py        # ADK Tool Functions (@tool)
        └── prompts.py      # System Instructions
```

---

## 4. Database Schema (Supabase)

Run the following SQL in the Supabase SQL Editor to initialize the persistence layer.

```sql
-- 1. Journal: The 'Memory' of the swarm.
-- The Dashboard subscribes to INSERTs on this table for the live feed.
create table journal (
  id bigint generated by default as identity primary key,
  created_at timestamp with time zone default timezone('utc'::text, now()) not null,
  agent_name text not null,     -- e.g. "Wilson", "Scanner"
  category text not null,       -- e.g. "Trade", "Error", "Thinking", "Signal"
  content text not null,        -- The actual message or JSON payload
  meta jsonb                    -- Optional structured data (ticker, price, etc)
);

-- 2. Watchlist: The current state of tracked assets.
create table watchlist (
  ticker text primary key,
  status text not null,         -- "Watching", "Long", "Short"
  sentiment_score numeric,      -- 0 to 100
  last_updated timestamp with time zone default now()
);

-- 3. Bot Config: Remote control switches.
create table bot_config (
  key text primary key,
  value text,
  description text
);

-- Seed initial config
INSERT INTO bot_config (key, value, description) 
VALUES ('trading_enabled', 'true', 'Master switch for trade execution');
```

---

## 5. Component Implementation Details

### 5.1. `app/extensions.py`

*Initialize shared extensions here to avoid circular imports.*

```python
from flask_apscheduler import APScheduler
from supabase import create_client, Client
import os

# Scheduler Instance
scheduler = APScheduler()

# Supabase Client (Lazy load to prevent import errors)
supabase: Client = None

def init_supabase(app):
    global supabase
    url = app.config.get("SUPABASE_URL")
    key = app.config.get("SUPABASE_KEY")
    if url and key:
        supabase = create_client(url, key)
```

### 5.2. `app/agents/tools.py`

*The "Hands" of the bot. Defined using ADK decorators.*

```python
from google.adk.tools import tool
from app.extensions import supabase
import requests

@tool
def log_journal(agent: str, category: str, content: str) -> str:
    """Logs an event to the cloud database.
    
    Args:
        agent: Name of the agent (e.g., 'Wilson').
        category: Type of log (Trade, Error, Summary).
        content: The message to log.
    """
    try:
        data = {"agent_name": agent, "category": category, "content": content}
        supabase.table("journal").insert(data).execute()
        return "Logged successfully."
    except Exception as e:
        return f"Log failed: {e}"

@tool
def check_market_status() -> str:
    """Checks if the US stock market is currently open."""
    # (Implement pandas_market_calendars logic here)
    return "Market is OPEN"
```

### 5.3. `app/agents/wilson.py`

*The "Brain". Defined using the Google ADK.*

```python
import os
from google.adk.agents import Agent
from google.adk.model import Model
from app.agents.tools import log_journal, check_market_status

# Configure the Model
model = Model(
    model_name="gemini-2.0-pro-exp",
    api_key=os.environ.get("GEMINI_API_KEY")
)

# Define the Orchestrator
wilson = Agent(
    name="Wilson",
    model=model,
    intro="""You are the Lead Trader of an autonomous swarm.
    Your capabilities:
    1. Orchestrate daily market scans.
    2. Delegate analysis to tools.
    3. Make final BUY/SELL decisions.
    
    Constraints:
    - You must always check if the market is open before trading.
    - You must log every major decision to the Journal.""",
    tools=[log_journal, check_market_status]
)
```

### 5.4. `app/tasks.py`

*The "Clock". Links Time to Action.*

```python
from app.extensions import scheduler
from app.agents.wilson import wilson

@scheduler.task('cron', id='morning_briefing', day_of_week='mon-fri', hour=8, minute=30)
def task_morning_briefing():
    """Runs at 8:30 AM ET on Weekdays."""
    print("⏰ Trigger: Morning Briefing")
    
    prompt = "It is 8:30 AM. Perform the pre-market scan and log your outlook."
    
    # Run the Agent
    try:
        response = wilson.run(prompt)
        print(f"Wilson Finished: {response.text}")
    except Exception as e:
        print(f"Wilson Crashed: {e}")

@scheduler.task('cron', id='market_monitor', day_of_week='mon-fri', hour='9-16', minute='*/15')
def task_market_monitor():
    """Runs every 15 mins during market hours."""
    print("⏰ Trigger: Market Monitor")
    wilson.run("Check the watchlist for breakout signals.")

```

### 5.5. `app/__init__.py`

*The Factory. Assembles the machine.*

```python
from flask import Flask
from app.extensions import scheduler, init_supabase
import os

def create_app():
    app = Flask(__name__)
    
    # Config
    app.config['SCHEDULER_API_ENABLED'] = True
    app.config['SUPABASE_URL'] = os.environ.get("SUPABASE_URL")
    app.config['SUPABASE_KEY'] = os.environ.get("SUPABASE_KEY")
    
    # Initialize Extensions
    init_supabase(app)
    scheduler.init_app(app)
    
    # Load Tasks (This registers the cron jobs)
    from app import tasks
    scheduler.start()
    
    # Routes
    @app.route("/")
    def index():
        return "DeepDiver Active. Check Supabase for logs."
        
    return app
```

---

## 6. Installation & Deployment

### 1. Hardware Prep

* Flash Raspberry Pi OS Lite (64-bit).
* Enable SSH.
* (Optional) Move filesystem to NVMe SSD for better durability.

### 2. Environment Setup

```bash
# Update System
sudo apt update && sudo apt upgrade -y

# Install Python & Venv
sudo apt install git python3-venv python3-pip -y

# Clone Repo (or create folder)
mkdir ~/deepdiver && cd ~/deepdiver

# Create Virtual Environment
python3 -m venv .venv
source .venv/bin/activate

# Install Dependencies
pip install flask flask-apscheduler google-adk supabase python-dotenv requests pandas_market_calendars
```

### 3. Service Configuration (`systemd`)

Create the service file to ensure the bot survives reboots.
`sudo nano /etc/systemd/system/deepdiver.service`

```ini
[Unit]
Description=DeepDiver Trading Bot
After=network.target

[Service]
User=pi
Group=pi
WorkingDirectory=/home/pi/deepdiver
Environment="PATH=/home/pi/deepdiver/.venv/bin"
EnvironmentFile=/home/pi/deepdiver/.env
ExecStart=/home/pi/deepdiver/.venv/bin/python run.py
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

### 4. Enable & Launch

```bash
sudo systemctl daemon-reload
sudo systemctl enable deepdiver
sudo systemctl start deepdiver
```

---

## 7. Operational Commands

* **View Logs:** `journalctl -u deepdiver -f`
* **Restart Bot:** `sudo systemctl restart deepdiver`
* **Stop Bot:** `sudo systemctl stop deepdiver`
* **Check Status:** `sudo systemctl status deepdiver`

